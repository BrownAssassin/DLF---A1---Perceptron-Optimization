{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.regularizers import l2, l1, l1_l2\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colours:\n",
    "    HEADER = '\\033[95m'     # Parameters\n",
    "    BLUE = '\\033[94m'       # Class Distribution\n",
    "    GREEN = '\\033[92m'      # Accuracies\n",
    "    RED = '\\033[91m'        # Loss\n",
    "    ENDC = '\\033[0m'        # End Colours\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading, Cleaning, & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "\n",
    "# Glucose: Replace '0' values with NaN\n",
    "data['Glucose'] = data['Glucose'].apply(lambda x: np.nan if x == 0 else x)\n",
    "\n",
    "# BloodPressure: Replace values below 40 mm Hg with NaN\n",
    "data['BloodPressure'] = data['BloodPressure'].apply(lambda x: np.nan if x < 40 else x)\n",
    "\n",
    "# SkinThickness: Replace values below 10 mm with NaN\n",
    "data['SkinThickness'] = data['SkinThickness'].apply(lambda x: np.nan if x < 10 else x)\n",
    "\n",
    "# Insulin: Replace values above 400 ÂµU/mL with NaN\n",
    "data['Insulin'] = data['Insulin'].apply(lambda x: np.nan if x > 400 else x)\n",
    "\n",
    "# BMI: Replace '0' values with NaN\n",
    "data['BMI'] = data['BMI'].apply(lambda x: np.nan if x == 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, testing, and validation datasets\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=42, stratify=y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Class distribution: {Colours.BLUE}{np.bincount(y)}{Colours.ENDC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data by performing KNN Imputation and scaling the dataset with RobustScaler\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Impute and scale training dataset\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "\n",
    "# Impute and scale validation dataset\n",
    "X_val_imputed = imputer.transform(X_val)\n",
    "X_val_scaled = scaler.transform(X_val_imputed)\n",
    "\n",
    "# Impute and scale trainval dataset\n",
    "X_trainval_imputed = imputer.fit_transform(X_trainval)\n",
    "X_trainval_scaled = scaler.fit_transform(X_trainval_imputed)\n",
    "\n",
    "# Impute and scale testing dataset\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Classifier Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a dummy classifier pipeline\n",
    "dummyPipeline = Pipeline([\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('classifier', DummyClassifier(strategy='stratified'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Single Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a standard single-layer perceptron\n",
    "def ssl_perceptron(learning_rate=0.01):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(1, input_dim=X_train.shape[1], activation='linear'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Single Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the optimized single-layer perceptron\n",
    "def osl_perceptron(learning_rate=0.01, activation='binary', regularization=None, lambda_=0.01, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "\n",
    "    if regularization == 'l1':\n",
    "        kernel_regularizer = l1(lambda_)\n",
    "    elif regularization == 'l2':\n",
    "        kernel_regularizer = l2(lambda_)\n",
    "    elif regularization == 'elasticnet':\n",
    "        kernel_regularizer = l1_l2(l1=lambda_, l2=lambda_)\n",
    "    else:\n",
    "        kernel_regularizer = None\n",
    "    \n",
    "    if activation == 'binary':\n",
    "        model.add(Dense(1, input_dim=X_train.shape[1], activation='linear', kernel_regularizer=kernel_regularizer))\n",
    "    else:\n",
    "        model.add(Dense(1, input_dim=X_train.shape[1], activation=activation, kernel_regularizer=kernel_regularizer))\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        optimizer_instance = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer_instance = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer_instance,\n",
    "                  loss=BinaryCrossentropy(from_logits=True) if activation == 'binary' else BinaryCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Priliminary Model Training, Hyperparameter Tuning, & Validation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct hyperparameter tuning on the Optimized Single Layer Perceptron (OSLP) model\n",
    "\n",
    "# Hyperparameter space\n",
    "learning_rates = [0.001, 0.01]                      # Learning rate values to test\n",
    "epochs_list = [500, 750]                            # Number of iterations (epochs) to test\n",
    "activations = ['sigmoid', 'tanh', 'relu']           # Activation functions to test\n",
    "regularizations = [None, 'l1', 'l2', 'elasticnet']  # Regularization types to test\n",
    "lambda_values = [0.001, 0.01]                       # Regularization strengths to test\n",
    "optimizers = ['adam', 'sgd']                        # Optimization methods to test\n",
    "\n",
    "# Tuning metrics\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "# Hyperparameter tuning loop\n",
    "for learning_rate, epochs, activation, regularization, lambda_, optimizer in itertools.product(learning_rates, epochs_list, activations, regularizations, lambda_values, optimizers):\n",
    "    print(f\"Training with learning_rate={learning_rate}, epochs={epochs}, activation={activation}, regularization={regularization}, lambda_={lambda_}, optimizer={optimizer}\")\n",
    "\n",
    "    # Build and train the model\n",
    "    model = osl_perceptron(learning_rate=learning_rate, activation=activation, optimizer=optimizer, regularization=regularization, lambda_=lambda_)\n",
    "    model.fit(X_train_scaled, y_train, epochs=epochs, batch_size=32, verbose=0, validation_data=(X_val_scaled, y_val))\n",
    "\n",
    "    # Evaluate the model\n",
    "    val_loss, val_accuracy = model.evaluate(X_val_scaled, y_val, verbose=0)\n",
    "    print(f\"Validation Accuracy: {Colours.GREEN}{val_accuracy:.4f}{Colours.ENDC}, Validation Loss: {Colours.RED}{val_loss:.4f}{Colours.ENDC}\")\n",
    "\n",
    "    # Update best parameters if current model is better\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_params = {\n",
    "            'learning_rate': learning_rate,\n",
    "            'epochs': epochs,\n",
    "            'activation': activation,\n",
    "            'regularization': regularization,\n",
    "            ''\n",
    "            'optimizer': optimizer\n",
    "        }\n",
    "\n",
    "# Display the best parameters and accuracy\n",
    "print(f\"Best Parameters: {Colours.HEADER}{best_params}{Colours.ENDC}\")\n",
    "print(f\"Best Validation Accuracy: {Colours.GREEN}{best_accuracy:.4f}{Colours.ENDC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually train the OSLP after determining ideal parameters\n",
    "\n",
    "# Map best parameters to Perceptron parameters, instantiate, and train the Perceptron\n",
    "best_oslp = osl_perceptron(learning_rate=best_params['learning_rate'],\n",
    "                            activation=best_params['activation'],\n",
    "                            regularization=best_params['regularization'],\n",
    "                            lambda_=best_params['lambda_'],\n",
    "                            optimizer=best_params['optimizer'])\n",
    "\n",
    "train_record_oslp = best_oslp.fit(X_train_scaled, y_train, epochs=best_params['epochs'], batch_size=32, validation_data=(X_val_scaled, y_val), verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the OSLP model on the Validation dataset\n",
    "\n",
    "# Use the trained perceptron to predict on the validation set\n",
    "y_val_pred_oslp = (best_oslp.predict(X_val_scaled) > (0.0 if best_params['activation'] == 'binary' else 0.5)).astype(int).flatten()\n",
    "\n",
    "# Calculate the accuracy\n",
    "val_accuracy_oslp = accuracy_score(y_val, y_val_pred_oslp)\n",
    "print(f\"Validation Accuracy: {Colours.GREEN}{val_accuracy_oslp:.2f}{Colours.ENDC}\")\n",
    "\n",
    "# Calculate the balanced accuracy\n",
    "val_balanced_accuracy_oslp = balanced_accuracy_score(y_val, y_val_pred_oslp)\n",
    "print(f\"Validation Balanced Accuracy: {Colours.GREEN}{val_balanced_accuracy_oslp:.2f}{Colours.ENDC}\")\n",
    "\n",
    "# Display a classification report\n",
    "print(f\"{Colours.BOLD}Validation Classification Report:{Colours.ENDC}\")\n",
    "print(classification_report(y_val, y_val_pred_oslp, target_names=['Non-Diabetic', 'Diabetic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Single-Layer Perceptron (SSLP) Training\n",
    "sslp = ssl_perceptron(learning_rate=best_params['learning_rate'])\n",
    "\n",
    "train_record_sslp = sslp.fit(X_train_scaled, y_train, epochs=best_params['epochs'], batch_size=32, validation_data=(X_val_scaled, y_val), verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate SSLP\n",
    "\n",
    "# Use the trained perceptron to predict on the validation set\n",
    "y_val_pred_sslp = (sslp.predict(X_val_scaled) > 0.0).astype(int).flatten()\n",
    "\n",
    "# Calculate the accuracy\n",
    "val_accuracy_sslp = accuracy_score(y_val, y_val_pred_sslp)\n",
    "print(f\"Validation Accuracy: {Colours.GREEN}{val_accuracy_sslp:.2f}{Colours.ENDC}\")\n",
    "\n",
    "# Calculate the balanced accuracy\n",
    "val_balanced_accuracy_sslp = balanced_accuracy_score(y_val, y_val_pred_sslp)\n",
    "print(f\"Validation Balanced Accuracy: {Colours.GREEN}{val_balanced_accuracy_sslp:.2f}{Colours.ENDC}\")\n",
    "\n",
    "# Display a classification report\n",
    "print(f\"{Colours.BOLD}Validation Classification Report:{Colours.ENDC}\")\n",
    "print(classification_report(y_val, y_val_pred_sslp, target_names=['Non-Diabetic', 'Diabetic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Classifier\n",
    "\n",
    "# Train\n",
    "dummyPipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "dummy_val_pred = dummyPipeline.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"{Colours.BOLD}Dummy Validation Classification Report:{Colours.ENDC}\")\n",
    "print(classification_report(y_val, dummy_val_pred, target_names=['Non-Diabetic', 'Diabetic']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Training Result Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Training vs Validation Accuracy over Epochs (OSLP)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_record_oslp.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(train_record_oslp.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training vs Validation Accuracy (OSLP)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Training vs Validation Loss over Epochs (OSLP)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_record_oslp.history['loss'], label='Training Loss')\n",
    "plt.plot(train_record_oslp.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training vs Validation Loss (OSLP)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True vs Predicted Labels (OSLP)\n",
    "ConfusionMatrixDisplay.from_predictions(y_val, y_val_pred_oslp, display_labels=['Non-Diabetic', 'Diabetic'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Training vs Validation Accuracy over Epochs (SSLP)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_record_sslp.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(train_record_sslp.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training vs Validation Accuracy (SSLP)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Training vs Validation Loss over Epochs (SSLP)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_record_sslp.history['loss'], label='Training Loss')\n",
    "plt.plot(train_record_sslp.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training vs Validation Loss (SSLP)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True vs Predicted Labels (SSLP)\n",
    "ConfusionMatrixDisplay.from_predictions(y_val, y_val_pred_sslp, display_labels=['Non-Diabetic', 'Diabetic'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary Model Training, Testing, & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train the OSLP with the combined trainval dataset and ideal parameters\n",
    "\n",
    "# Map best parameters to Perceptron parameters, instantiate, and train the Perceptron\n",
    "final_oslp = osl_perceptron(learning_rate=best_params['learning_rate'],\n",
    "                            activation=best_params['activation'],\n",
    "                            regularization=best_params['regularization'],\n",
    "                            lambda_=best_params['lambda_'],\n",
    "                            optimizer=best_params['optimizer'])\n",
    "\n",
    "trainval_record_oslp = final_oslp.fit(X_trainval_scaled, y_trainval, epochs=best_params['epochs'], batch_size=32, validation_data=(X_test_scaled, y_test), verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the OSLP model on the Testing dataset\n",
    "\n",
    "# Use the re-trained perceptron to predict on the testing set\n",
    "y_test_pred_oslp = (final_oslp.predict(X_test_scaled) > (0.0 if best_params['activation'] == 'binary' else 0.5)).astype(int).flatten()\n",
    "\n",
    "# Calculate the accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred_oslp)\n",
    "print(f\"Testing Accuracy: {Colours.GREEN}{test_accuracy:.2f}{Colours.ENDC}\")\n",
    "\n",
    "# Calculate the balanced accuracy\n",
    "test_balanced_accuracy = balanced_accuracy_score(y_test, y_test_pred_oslp)\n",
    "print(f\"Testing Balanced Accuracy: {Colours.GREEN}{test_balanced_accuracy:.2f}{Colours.ENDC}\")\n",
    "\n",
    "print(f\"{Colours.BOLD}Testing Classification Report:{Colours.ENDC}\")\n",
    "print(classification_report(y_test, y_test_pred_oslp, target_names=['Non-Diabetic', 'Diabetic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Single-Layer Perceptron (SSLP) Re-Training\n",
    "final_sslp = ssl_perceptron(learning_rate=best_params['learning_rate'])\n",
    "\n",
    "trainval_record_sslp = final_sslp.fit(X_trainval_scaled, y_trainval, epochs=best_params['epochs'], batch_size=32, validation_data=(X_test_scaled, y_test), verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Evaluate SSLP\n",
    "\n",
    "# Use the re-trained perceptron to predict on the testing set\n",
    "y_test_pred_sslp = (final_sslp.predict(X_test_scaled) > 0.0).astype(int).flatten()\n",
    "\n",
    "# Calculate the accuracy\n",
    "test_accuracy_sslp = accuracy_score(y_test, y_test_pred_sslp)\n",
    "print(f\"Testing Accuracy: {Colours.GREEN}{test_accuracy_sslp:.2f}{Colours.ENDC}\")\n",
    "\n",
    "# Calculate the balanced accuracy\n",
    "test_balanced_accuracy_sslp = balanced_accuracy_score(y_test, y_test_pred_sslp)\n",
    "print(f\"Testing Balanced Accuracy: {Colours.GREEN}{test_balanced_accuracy_sslp:.2f}{Colours.ENDC}\")\n",
    "\n",
    "# Display a classification report\n",
    "print(f\"{Colours.BOLD}Testing Classification Report:{Colours.ENDC}\")\n",
    "print(classification_report(y_test, y_test_pred_sslp, target_names=['Non-Diabetic', 'Diabetic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Classifier\n",
    "\n",
    "# Train\n",
    "dummyPipeline.fit(X_trainval, y_trainval)\n",
    "\n",
    "# Predict\n",
    "dummy_test_pred = dummyPipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"{Colours.BOLD}Dummy Testing Classification Report:{Colours.ENDC}\")\n",
    "print(classification_report(y_test, dummy_test_pred, target_names=['Non-Diabetic', 'Diabetic']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary Training Result Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Training vs Testing Accuracy over Epochs (OSLP)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(trainval_record_oslp.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(trainval_record_oslp.history['val_accuracy'], label='Testing Accuracy')\n",
    "plt.title('Training vs Testing Accuracy (OSLP)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Training vs Testing Loss over Epochs (OSLP)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(trainval_record_oslp.history['loss'], label='Training Loss')\n",
    "plt.plot(trainval_record_oslp.history['val_loss'], label='Testing Loss')\n",
    "plt.title('Training vs Testing Loss (OSLP)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True vs Predicted Labels (OSLP)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred_oslp, display_labels=['Non-Diabetic', 'Diabetic'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Training vs Testing Accuracy over Epochs (SSLP)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(trainval_record_sslp.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(trainval_record_sslp.history['val_accuracy'], label='Testing Accuracy')\n",
    "plt.title('Training vs Testing Accuracy (SSLP)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Training vs Testing Loss over Epochs (SSLP)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(trainval_record_sslp.history['loss'], label='Training Loss')\n",
    "plt.plot(trainval_record_sslp.history['val_loss'], label='Testing Loss')\n",
    "plt.title('Training vs Testing Loss (SSLP)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True vs Predicted Labels (SSLP)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred_sslp, display_labels=['Non-Diabetic', 'Diabetic'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
